{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaixinLiuNeuro/ALBEF/blob/main/colab_load_pretrained4M_freezeFineTuneVQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24nSaNVhFajG"
      },
      "source": [
        "# Load pretrained 4M model, freeze encoder's parameters fine-tune with only VQA dataset, run evaluation test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jpeIswepIcEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734f8cf9-032b-46c7-b578-db2984eea1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-C2cjy7sLsaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c44ca70-0486-4e9d-f308-6309ebaf2e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['output', 'vqa_end2end', 'vqa_onlyPretrainModel', 'vqa_nopretrain_noTune']\n"
          ]
        }
      ],
      "source": [
        "# setup drive folder\n",
        "import os\n",
        "\n",
        "# TODO: Fill in the Google Drive path where you want to save result\n",
        "GOOGLE_DRIVE_PATH_POST_MYDRIVE = os.path.join('DL_Project', 'ALBEF')\n",
        "GOOGLE_DRIVE_PATH = os.path.join('/content', 'drive', 'MyDrive', GOOGLE_DRIVE_PATH_POST_MYDRIVE)\n",
        "os.makedirs(GOOGLE_DRIVE_PATH, exist_ok=True)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3gnMy6WHMjZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8d2899-4ccd-4ae3-87ab-f5263b820b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in google colab. Our path is `/content/drive/MyDrive/DL_Project/ALBEF`\n"
          ]
        }
      ],
      "source": [
        "# if running locally set GOOGLE PATH\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  print(f'Running in google colab. Our path is `{GOOGLE_DRIVE_PATH}`')\n",
        "else:\n",
        "  GOOGLE_DRIVE_PATH = '.'\n",
        "  print('Running locally.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7vaWO3M8Mmxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f95c1f9-cfe6-4963-eec8-14292fc90203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive Path: /content/drive/MyDrive/DL_Project/ALBEF\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import math\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "print(f'Google Drive Path: {GOOGLE_DRIVE_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cj6rXs28G5ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4209deae-b087-4702-b81d-9a3e71e9f6a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/tmp/ALBEF'...\n",
            "remote: Enumerating objects: 405, done.\u001b[K\n",
            "remote: Counting objects: 100% (243/243), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 405 (delta 140), reused 124 (delta 122), pack-reused 162 (from 2)\u001b[K\n",
            "Receiving objects: 100% (405/405), 71.62 MiB | 63.99 MiB/s, done.\n",
            "Resolving deltas: 100% (164/164), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repo to a content\n",
        "!git clone -b main https://github.com/HaixinLiuNeuro/ALBEF.git /tmp/ALBEF\n",
        "!cp -r /tmp/ALBEF/* .\n",
        "!rm -rf /tmp/ALBEF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "N_7q2p11MyGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5224ba-c219-415e-c932-965da57623ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.25.1\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl.metadata (93 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.25.1)\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1) (2025.1.31)\n",
            "Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.25.1\n",
            "Collecting ruamel.yaml==0.17.*\n",
            "  Downloading ruamel.yaml-0.17.40-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml==0.17.*)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading ruamel.yaml-0.17.40-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.17.40 ruamel.yaml.clib-0.2.12\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# install dependency\n",
        "!pip install transformers==4.25.1\n",
        "!pip install ruamel.yaml==0.17.*\n",
        "!pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nQxFlSxfFajH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0277d9d-0927-4394-c687-b3d563334259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# import\n",
        "import argparse\n",
        "import os\n",
        "import ruamel.yaml as yaml\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "\n",
        "# use vqa model\n",
        "from models.model_vqa_freeze import ALBEF\n",
        "\n",
        "from models.vit import interpolate_pos_embed\n",
        "from models.tokenization_bert import BertTokenizer\n",
        "\n",
        "import utils\n",
        "from dataset.utils import save_result\n",
        "from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn\n",
        "\n",
        "from scheduler import create_scheduler\n",
        "from optim import create_optimizer\n",
        "\n",
        "# print and plotting\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JiebcG8heA07"
      },
      "outputs": [],
      "source": [
        "# %reload_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kPxGkhRy4FLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230a3050-5030-4322-97cc-22ad715cad16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "Downloading data.tar.gz...\n",
            "data.tar.gz         100%[===================>] 137.87M   237MB/s    in 0.6s    \n",
            "Extracting data.tar.gz...\n",
            "Removing data.tar.gz...\n",
            "Finished processing data.tar.gz\n",
            "Downloading train2014.zip...\n",
            "train2014.zip       100%[===================>]  12.58G  50.5MB/s    in 2m 45s  \n",
            "Extracting train2014.zip...\n",
            "Removing train2014.zip...\n",
            "Finished processing train2014.zip\n",
            "Downloading val2014.zip...\n",
            "val2014.zip         100%[===================>]   6.19G  64.6MB/s    in 1m 40s  \n",
            "Extracting val2014.zip...\n",
            "Removing val2014.zip...\n",
            "Finished processing val2014.zip\n",
            "Downloading test2015.zip...\n",
            "test2015.zip        100%[===================>]  12.36G  51.3MB/s    in 4m 3s   \n",
            "Extracting test2015.zip...\n",
            "Removing test2015.zip...\n",
            "Finished processing test2015.zip\n",
            "All downloads and extractions completed!\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# prep data\n",
        "# download from website\n",
        "\n",
        "# make folder /content/data\n",
        "DATA_PATH = os.path.join('/content', 'data')\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "%cd /content/data\n",
        "\n",
        "# download data from links:\n",
        "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/json_pretrain.zip\n",
        "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/data.tar.gz\n",
        "# http://images.cocodataset.org/zips/train2014.zip\n",
        "# http://images.cocodataset.org/zips/val2014.zip\n",
        "# http://images.cocodataset.org/zips/test2015.zip\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the download links\n",
        "links = [\n",
        "    # \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/json_pretrain.zip\", # pretrain json\n",
        "    \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/data.tar.gz\", # for downstream task json\n",
        "    \"http://images.cocodataset.org/zips/train2014.zip\", # comment out if only run evaluation\n",
        "    \"http://images.cocodataset.org/zips/val2014.zip\",   # comment out if only run evaluation\n",
        "    \"http://images.cocodataset.org/zips/test2015.zip\"\n",
        "]\n",
        "\n",
        "# Download and extract each file\n",
        "for link in links:\n",
        "    filename = link.split('/')[-1]\n",
        "    print(f\"Downloading {filename}...\")\n",
        "\n",
        "    # Download file\n",
        "    !wget -q --show-progress {link}\n",
        "\n",
        "    print(f\"Extracting {filename}...\")\n",
        "\n",
        "    # Extract based on file extension\n",
        "    if filename.endswith('.zip'):\n",
        "      if '//images.cocodataset.org/zips/' in link:\n",
        "        !unzip -q {filename}\n",
        "      else:\n",
        "        !unzip -q -j {filename}  # -j option flattens the directory structure for json_pretrain.zip\n",
        "    elif filename.endswith('.tar.gz'):\n",
        "        !tar -xzf {filename} --strip-components=1  # Remove the top-level directory\n",
        "\n",
        "    # Delete the zip/tar file after extraction\n",
        "    print(f\"Removing {filename}...\")\n",
        "    !rm {filename}\n",
        "\n",
        "    print(f\"Finished processing {filename}\")\n",
        "\n",
        "print(\"All downloads and extractions completed!\")\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Sc9XbDXyL4WJ"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y9cbgfXjUT_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85406a1-7def-4d2d-b231-32f4d86e1cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "answer_list.json      nlvr_dev.json\t   test2015\t  vqa_test_dev.json\n",
            "coco_test.json\t      nlvr_test.json\t   train2014\t  vqa_test.json\n",
            "coco_train.json       nlvr_train.json\t   val2014\t  vqa_train.json\n",
            "coco_val.json\t      refcoco+\t\t   ve_dev.json\t  vqa_val.json\n",
            "flickr30k_test.json   refcoco+_test.json   ve_test.json\n",
            "flickr30k_train.json  refcoco+_train.json  ve_train.json\n",
            "flickr30k_val.json    refcoco+_val.json    vg_qa.json\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# check files\n",
        "%cd /content/data\n",
        "!ls\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wWKM1ISxSrPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c8fea3-6729-4748-9535-e5f871dc7058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading ALBEF_4M.pth...\n",
            "ALBEF_4M.pth        100%[===================>]   3.25G   134MB/s    in 17s     \n",
            "Finished processing ALBEF_4M.pth\n",
            "All model downloads completed!\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "FETCH_PRETRAINED_MODEL = True\n",
        "%cd /content\n",
        "\n",
        "# download data from links:\n",
        "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\n",
        "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/vqa.pth\n",
        "# model check point from training\n",
        "# https://drive.google.com/file/d/1yEsyeB0FkIgWlT2Way_KFLPNLCQy6KoU/view?usp=sharing\n",
        "\n",
        "if FETCH_PRETRAINED_MODEL:\n",
        "\n",
        "  # Define the download links\n",
        "  links = [\n",
        "      \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\",\n",
        "      # \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/vqa.pth\"\n",
        "  ]\n",
        "\n",
        "  # Download and extract each file\n",
        "  for link in links:\n",
        "      filename = link.split('/')[-1]\n",
        "      print(f\"Downloading {filename}...\")\n",
        "\n",
        "      # Download file\n",
        "      !wget -q --show-progress {link}\n",
        "\n",
        "\n",
        "      print(f\"Finished processing {filename}\")\n",
        "\n",
        "  print(\"All model downloads completed!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm5adbe9FajI"
      },
      "source": [
        "## Setup for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1SSGJE2MFajI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555566ce-c8b3-4022-bae8-afe5f6764698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "{'alpha': 0.4,\n",
            " 'answer_list': 'data/answer_list.json',\n",
            " 'batch_size_test': 64,\n",
            " 'batch_size_train': 32,\n",
            " 'bert_config': 'configs/config_bert.json',\n",
            " 'distill': True,\n",
            " 'eos': '[SEP]',\n",
            " 'image_res': 384,\n",
            " 'k_test': 128,\n",
            " 'optimizer': {'lr': 2e-05, 'opt': 'adamW', 'weight_decay': 0.02},\n",
            " 'schedular': {'cooldown_epochs': 0,\n",
            "               'decay_rate': 1,\n",
            "               'epochs': 8,\n",
            "               'lr': 2e-05,\n",
            "               'min_lr': 1e-06,\n",
            "               'sched': 'cosine',\n",
            "               'warmup_epochs': 4,\n",
            "               'warmup_lr': 1e-05},\n",
            " 'test_file': ['data/vqa_test.json'],\n",
            " 'train_file': ['data/vqa_train.json', 'data/vqa_val.json'],\n",
            " 'vg_root': 'data/',\n",
            " 'vqa_root': 'data/',\n",
            " 'warm_up': True}\n"
          ]
        }
      ],
      "source": [
        "# config\n",
        "%cd /content\n",
        "args = argparse.Namespace()\n",
        "args.config = './configs/VQA_biggerTestBatch.yaml'\n",
        "args.checkpoint = './ALBEF_4M.pth'\n",
        "args.output_dir = 'output/vqa_PretrainModel_freezeTune'\n",
        "args.evaluate = True # to train use False\n",
        "args.text_encoder = 'bert-base-uncased'\n",
        "args.text_decoder = 'bert-base-uncased'\n",
        "args.device = 'cuda'\n",
        "args.seed = 42\n",
        "args.distributed = False\n",
        "\n",
        "config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n",
        "pprint(config)\n",
        "\n",
        "# make result folder and save config\n",
        "args.result_dir = os.path.join(args.output_dir, 'result')\n",
        "\n",
        "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gyGrj0NZFajI"
      },
      "outputs": [],
      "source": [
        "# training functions\n",
        "def train(model, data_loader, optimizer, tokenizer, epoch, warmup_steps, device, scheduler, config):\n",
        "    # train\n",
        "    model.train()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
        "\n",
        "    header = 'Train Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 50\n",
        "    step_size = 100\n",
        "    warmup_iterations = warmup_steps*step_size\n",
        "\n",
        "    for i,(image, question, answer, weights, n) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
        "        image, weights = image.to(device,non_blocking=True), weights.to(device,non_blocking=True)\n",
        "        question_input = tokenizer(question, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\").to(device)\n",
        "        answer_input = tokenizer(answer, padding='longest', return_tensors=\"pt\").to(device)\n",
        "\n",
        "        if epoch>0 or not config['warm_up']:\n",
        "            alpha = config['alpha']\n",
        "        else:\n",
        "            alpha = config['alpha']*min(1,i/len(data_loader))\n",
        "\n",
        "        loss = model(image, question_input, answer_input, train=True, alpha=alpha, k=n, weights=weights)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss=loss.item())\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        if epoch==0 and i%step_size==0 and i<=warmup_iterations:\n",
        "            scheduler.step(i//step_size)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger.global_avg())\n",
        "    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluation(model, data_loader, tokenizer, device, config) :\n",
        "    # test\n",
        "    model.eval()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Generate VQA test result:'\n",
        "    print_freq = 50\n",
        "\n",
        "    result = []\n",
        "\n",
        "    answer_list = [answer+config['eos'] for answer in data_loader.dataset.answer_list]\n",
        "    answer_input = tokenizer(answer_list, padding='longest', return_tensors='pt').to(device)\n",
        "\n",
        "    for n, (image, question, question_id) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
        "        image = image.to(device,non_blocking=True)\n",
        "        question_input = tokenizer(question, padding='longest', return_tensors=\"pt\").to(device)\n",
        "\n",
        "        topk_ids, topk_probs = model(image, question_input, answer_input, train=False, k=config['k_test'])\n",
        "\n",
        "        for ques_id, topk_id, topk_prob in zip(question_id, topk_ids, topk_probs):\n",
        "            ques_id = int(ques_id.item())\n",
        "            _, pred = topk_prob.max(dim=0)\n",
        "            result.append({\"question_id\":ques_id, \"answer\":data_loader.dataset.answer_list[topk_id[pred]]})\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "O_fo-oPwFajI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1021383-0511-4480-a4e5-7e4c38a7020c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "# setup for training/evaluation (from main)\n",
        "utils.init_distributed_mode(args)\n",
        "\n",
        "device = torch.device(args.device)\n",
        "print(f'device: {device}')\n",
        "\n",
        "# fix the seed for reproducibility\n",
        "seed = args.seed + utils.get_rank()\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "start_epoch = 0\n",
        "max_epoch = config['schedular']['epochs']\n",
        "warmup_steps = config['schedular']['warmup_epochs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lmtKe-XVFajI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfcc9db-8f8c-4467-8560-0841bc5a46db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vqa datasets\n"
          ]
        }
      ],
      "source": [
        "# make dataset and dataloader\n",
        "print(\"Creating vqa datasets\")\n",
        "datasets = create_dataset('vqa', config)\n",
        "\n",
        "if args.distributed:\n",
        "    num_tasks = utils.get_world_size()\n",
        "    global_rank = utils.get_rank()\n",
        "    samplers = create_sampler(datasets, [True, False], num_tasks, global_rank)\n",
        "else:\n",
        "    samplers = [None, None]\n",
        "\n",
        "train_loader, test_loader = create_loader(datasets,samplers,\n",
        "                                          batch_size=[config['batch_size_train'],config['batch_size_test']],\n",
        "                                          num_workers=[4,4],is_trains=[True, False],\n",
        "                                          collate_fns=[vqa_collate_fn,None])\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(args.text_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6zYcjMhhFajI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac508287-39bc-414c-bd06-154a7cbc0845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ALBEF(\n",
              "  (visual_encoder): VisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (blocks): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (text_encoder): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6-11): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (crossattention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (text_decoder): BertLMHeadModel(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-5): 6 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cls): BertOnlyMLMHead(\n",
              "      (predictions): BertLMPredictionHead(\n",
              "        (transform): BertPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (visual_encoder_m): VisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (blocks): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (text_encoder_m): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6-11): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (crossattention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (text_decoder_m): BertLMHeadModel(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-5): 6 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cls): BertOnlyMLMHead(\n",
              "      (predictions): BertLMPredictionHead(\n",
              "        (transform): BertPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#### Model ####\n",
        "print(\"Creating model\")\n",
        "model = ALBEF(config=config, text_encoder=args.text_encoder, text_decoder=args.text_decoder, tokenizer=tokenizer)\n",
        "model = model.to(device)\n",
        "\n",
        "arg_opt = utils.AttrDict(config['optimizer'])\n",
        "optimizer = create_optimizer(arg_opt, model)\n",
        "arg_sche = utils.AttrDict(config['schedular'])\n",
        "lr_scheduler, _ = create_scheduler(arg_sche, optimizer)\n",
        "\n",
        "# check model\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "B64EarwbFajJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46079b24-7bd0-4db9-eaa0-16a4565600a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'visual_encoder.pos_embed' not found in checkpoint. Skipping positional embedding interpolation.\n",
            "load checkpoint from ./ALBEF_4M.pth\n",
            "_IncompatibleKeys(missing_keys=['visual_encoder.cls_token', 'visual_encoder.pos_embed', 'visual_encoder.patch_embed.proj.weight', 'visual_encoder.patch_embed.proj.bias', 'visual_encoder.blocks.0.norm1.weight', 'visual_encoder.blocks.0.norm1.bias', 'visual_encoder.blocks.0.attn.qkv.weight', 'visual_encoder.blocks.0.attn.qkv.bias', 'visual_encoder.blocks.0.attn.proj.weight', 'visual_encoder.blocks.0.attn.proj.bias', 'visual_encoder.blocks.0.norm2.weight', 'visual_encoder.blocks.0.norm2.bias', 'visual_encoder.blocks.0.mlp.fc1.weight', 'visual_encoder.blocks.0.mlp.fc1.bias', 'visual_encoder.blocks.0.mlp.fc2.weight', 'visual_encoder.blocks.0.mlp.fc2.bias', 'visual_encoder.blocks.1.norm1.weight', 'visual_encoder.blocks.1.norm1.bias', 'visual_encoder.blocks.1.attn.qkv.weight', 'visual_encoder.blocks.1.attn.qkv.bias', 'visual_encoder.blocks.1.attn.proj.weight', 'visual_encoder.blocks.1.attn.proj.bias', 'visual_encoder.blocks.1.norm2.weight', 'visual_encoder.blocks.1.norm2.bias', 'visual_encoder.blocks.1.mlp.fc1.weight', 'visual_encoder.blocks.1.mlp.fc1.bias', 'visual_encoder.blocks.1.mlp.fc2.weight', 'visual_encoder.blocks.1.mlp.fc2.bias', 'visual_encoder.blocks.2.norm1.weight', 'visual_encoder.blocks.2.norm1.bias', 'visual_encoder.blocks.2.attn.qkv.weight', 'visual_encoder.blocks.2.attn.qkv.bias', 'visual_encoder.blocks.2.attn.proj.weight', 'visual_encoder.blocks.2.attn.proj.bias', 'visual_encoder.blocks.2.norm2.weight', 'visual_encoder.blocks.2.norm2.bias', 'visual_encoder.blocks.2.mlp.fc1.weight', 'visual_encoder.blocks.2.mlp.fc1.bias', 'visual_encoder.blocks.2.mlp.fc2.weight', 'visual_encoder.blocks.2.mlp.fc2.bias', 'visual_encoder.blocks.3.norm1.weight', 'visual_encoder.blocks.3.norm1.bias', 'visual_encoder.blocks.3.attn.qkv.weight', 'visual_encoder.blocks.3.attn.qkv.bias', 'visual_encoder.blocks.3.attn.proj.weight', 'visual_encoder.blocks.3.attn.proj.bias', 'visual_encoder.blocks.3.norm2.weight', 'visual_encoder.blocks.3.norm2.bias', 'visual_encoder.blocks.3.mlp.fc1.weight', 'visual_encoder.blocks.3.mlp.fc1.bias', 'visual_encoder.blocks.3.mlp.fc2.weight', 'visual_encoder.blocks.3.mlp.fc2.bias', 'visual_encoder.blocks.4.norm1.weight', 'visual_encoder.blocks.4.norm1.bias', 'visual_encoder.blocks.4.attn.qkv.weight', 'visual_encoder.blocks.4.attn.qkv.bias', 'visual_encoder.blocks.4.attn.proj.weight', 'visual_encoder.blocks.4.attn.proj.bias', 'visual_encoder.blocks.4.norm2.weight', 'visual_encoder.blocks.4.norm2.bias', 'visual_encoder.blocks.4.mlp.fc1.weight', 'visual_encoder.blocks.4.mlp.fc1.bias', 'visual_encoder.blocks.4.mlp.fc2.weight', 'visual_encoder.blocks.4.mlp.fc2.bias', 'visual_encoder.blocks.5.norm1.weight', 'visual_encoder.blocks.5.norm1.bias', 'visual_encoder.blocks.5.attn.qkv.weight', 'visual_encoder.blocks.5.attn.qkv.bias', 'visual_encoder.blocks.5.attn.proj.weight', 'visual_encoder.blocks.5.attn.proj.bias', 'visual_encoder.blocks.5.norm2.weight', 'visual_encoder.blocks.5.norm2.bias', 'visual_encoder.blocks.5.mlp.fc1.weight', 'visual_encoder.blocks.5.mlp.fc1.bias', 'visual_encoder.blocks.5.mlp.fc2.weight', 'visual_encoder.blocks.5.mlp.fc2.bias', 'visual_encoder.blocks.6.norm1.weight', 'visual_encoder.blocks.6.norm1.bias', 'visual_encoder.blocks.6.attn.qkv.weight', 'visual_encoder.blocks.6.attn.qkv.bias', 'visual_encoder.blocks.6.attn.proj.weight', 'visual_encoder.blocks.6.attn.proj.bias', 'visual_encoder.blocks.6.norm2.weight', 'visual_encoder.blocks.6.norm2.bias', 'visual_encoder.blocks.6.mlp.fc1.weight', 'visual_encoder.blocks.6.mlp.fc1.bias', 'visual_encoder.blocks.6.mlp.fc2.weight', 'visual_encoder.blocks.6.mlp.fc2.bias', 'visual_encoder.blocks.7.norm1.weight', 'visual_encoder.blocks.7.norm1.bias', 'visual_encoder.blocks.7.attn.qkv.weight', 'visual_encoder.blocks.7.attn.qkv.bias', 'visual_encoder.blocks.7.attn.proj.weight', 'visual_encoder.blocks.7.attn.proj.bias', 'visual_encoder.blocks.7.norm2.weight', 'visual_encoder.blocks.7.norm2.bias', 'visual_encoder.blocks.7.mlp.fc1.weight', 'visual_encoder.blocks.7.mlp.fc1.bias', 'visual_encoder.blocks.7.mlp.fc2.weight', 'visual_encoder.blocks.7.mlp.fc2.bias', 'visual_encoder.blocks.8.norm1.weight', 'visual_encoder.blocks.8.norm1.bias', 'visual_encoder.blocks.8.attn.qkv.weight', 'visual_encoder.blocks.8.attn.qkv.bias', 'visual_encoder.blocks.8.attn.proj.weight', 'visual_encoder.blocks.8.attn.proj.bias', 'visual_encoder.blocks.8.norm2.weight', 'visual_encoder.blocks.8.norm2.bias', 'visual_encoder.blocks.8.mlp.fc1.weight', 'visual_encoder.blocks.8.mlp.fc1.bias', 'visual_encoder.blocks.8.mlp.fc2.weight', 'visual_encoder.blocks.8.mlp.fc2.bias', 'visual_encoder.blocks.9.norm1.weight', 'visual_encoder.blocks.9.norm1.bias', 'visual_encoder.blocks.9.attn.qkv.weight', 'visual_encoder.blocks.9.attn.qkv.bias', 'visual_encoder.blocks.9.attn.proj.weight', 'visual_encoder.blocks.9.attn.proj.bias', 'visual_encoder.blocks.9.norm2.weight', 'visual_encoder.blocks.9.norm2.bias', 'visual_encoder.blocks.9.mlp.fc1.weight', 'visual_encoder.blocks.9.mlp.fc1.bias', 'visual_encoder.blocks.9.mlp.fc2.weight', 'visual_encoder.blocks.9.mlp.fc2.bias', 'visual_encoder.blocks.10.norm1.weight', 'visual_encoder.blocks.10.norm1.bias', 'visual_encoder.blocks.10.attn.qkv.weight', 'visual_encoder.blocks.10.attn.qkv.bias', 'visual_encoder.blocks.10.attn.proj.weight', 'visual_encoder.blocks.10.attn.proj.bias', 'visual_encoder.blocks.10.norm2.weight', 'visual_encoder.blocks.10.norm2.bias', 'visual_encoder.blocks.10.mlp.fc1.weight', 'visual_encoder.blocks.10.mlp.fc1.bias', 'visual_encoder.blocks.10.mlp.fc2.weight', 'visual_encoder.blocks.10.mlp.fc2.bias', 'visual_encoder.blocks.11.norm1.weight', 'visual_encoder.blocks.11.norm1.bias', 'visual_encoder.blocks.11.attn.qkv.weight', 'visual_encoder.blocks.11.attn.qkv.bias', 'visual_encoder.blocks.11.attn.proj.weight', 'visual_encoder.blocks.11.attn.proj.bias', 'visual_encoder.blocks.11.norm2.weight', 'visual_encoder.blocks.11.norm2.bias', 'visual_encoder.blocks.11.mlp.fc1.weight', 'visual_encoder.blocks.11.mlp.fc1.bias', 'visual_encoder.blocks.11.mlp.fc2.weight', 'visual_encoder.blocks.11.mlp.fc2.bias', 'visual_encoder.norm.weight', 'visual_encoder.norm.bias', 'text_encoder.embeddings.position_ids', 'text_encoder.embeddings.word_embeddings.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.token_type_embeddings.weight', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.crossattention.self.key.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.crossattention.self.query.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.weight', 'text_encoder.encoder.layer.7.crossattention.self.key.bias', 'text_encoder.encoder.layer.7.crossattention.self.value.weight', 'text_encoder.encoder.layer.7.crossattention.self.value.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.crossattention.self.query.weight', 'text_encoder.encoder.layer.8.crossattention.self.query.bias', 'text_encoder.encoder.layer.8.crossattention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.crossattention.self.value.weight', 'text_encoder.encoder.layer.8.crossattention.self.value.bias', 'text_encoder.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'text_decoder.bert.embeddings.position_ids', 'text_decoder.bert.embeddings.word_embeddings.weight', 'text_decoder.bert.embeddings.position_embeddings.weight', 'text_decoder.bert.embeddings.token_type_embeddings.weight', 'text_decoder.bert.embeddings.LayerNorm.weight', 'text_decoder.bert.embeddings.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.bias', 'text_decoder.bert.encoder.layer.0.attention.self.key.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.weight', 'text_decoder.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.0.output.dense.weight', 'text_decoder.bert.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.attention.self.query.weight', 'text_decoder.bert.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.self.key.weight', 'text_decoder.bert.encoder.layer.1.attention.self.key.bias', 'text_decoder.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.bias', 'text_decoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.1.output.dense.bias', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.weight', 'text_decoder.bert.encoder.layer.2.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.self.value.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.bias', 'text_decoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.output.dense.weight', 'text_decoder.bert.encoder.layer.2.output.dense.bias', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.self.query.weight', 'text_decoder.bert.encoder.layer.3.attention.self.query.bias', 'text_decoder.bert.encoder.layer.3.attention.self.key.weight', 'text_decoder.bert.encoder.layer.3.attention.self.key.bias', 'text_decoder.bert.encoder.layer.3.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.attention.self.value.bias', 'text_decoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.output.dense.weight', 'text_decoder.bert.encoder.layer.3.output.dense.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.self.query.bias', 'text_decoder.bert.encoder.layer.4.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.bias', 'text_decoder.bert.encoder.layer.4.attention.self.value.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.bias', 'text_decoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.4.output.dense.weight', 'text_decoder.bert.encoder.layer.4.output.dense.bias', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.attention.self.query.bias', 'text_decoder.bert.encoder.layer.5.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.self.key.bias', 'text_decoder.bert.encoder.layer.5.attention.self.value.weight', 'text_decoder.bert.encoder.layer.5.attention.self.value.bias', 'text_decoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.5.output.dense.weight', 'text_decoder.bert.encoder.layer.5.output.dense.bias', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_decoder.cls.predictions.bias', 'text_decoder.cls.predictions.transform.dense.weight', 'text_decoder.cls.predictions.transform.dense.bias', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.cls.predictions.decoder.bias', 'visual_encoder_m.cls_token', 'visual_encoder_m.pos_embed', 'visual_encoder_m.patch_embed.proj.weight', 'visual_encoder_m.patch_embed.proj.bias', 'visual_encoder_m.blocks.0.norm1.weight', 'visual_encoder_m.blocks.0.norm1.bias', 'visual_encoder_m.blocks.0.attn.qkv.weight', 'visual_encoder_m.blocks.0.attn.qkv.bias', 'visual_encoder_m.blocks.0.attn.proj.weight', 'visual_encoder_m.blocks.0.attn.proj.bias', 'visual_encoder_m.blocks.0.norm2.weight', 'visual_encoder_m.blocks.0.norm2.bias', 'visual_encoder_m.blocks.0.mlp.fc1.weight', 'visual_encoder_m.blocks.0.mlp.fc1.bias', 'visual_encoder_m.blocks.0.mlp.fc2.weight', 'visual_encoder_m.blocks.0.mlp.fc2.bias', 'visual_encoder_m.blocks.1.norm1.weight', 'visual_encoder_m.blocks.1.norm1.bias', 'visual_encoder_m.blocks.1.attn.qkv.weight', 'visual_encoder_m.blocks.1.attn.qkv.bias', 'visual_encoder_m.blocks.1.attn.proj.weight', 'visual_encoder_m.blocks.1.attn.proj.bias', 'visual_encoder_m.blocks.1.norm2.weight', 'visual_encoder_m.blocks.1.norm2.bias', 'visual_encoder_m.blocks.1.mlp.fc1.weight', 'visual_encoder_m.blocks.1.mlp.fc1.bias', 'visual_encoder_m.blocks.1.mlp.fc2.weight', 'visual_encoder_m.blocks.1.mlp.fc2.bias', 'visual_encoder_m.blocks.2.norm1.weight', 'visual_encoder_m.blocks.2.norm1.bias', 'visual_encoder_m.blocks.2.attn.qkv.weight', 'visual_encoder_m.blocks.2.attn.qkv.bias', 'visual_encoder_m.blocks.2.attn.proj.weight', 'visual_encoder_m.blocks.2.attn.proj.bias', 'visual_encoder_m.blocks.2.norm2.weight', 'visual_encoder_m.blocks.2.norm2.bias', 'visual_encoder_m.blocks.2.mlp.fc1.weight', 'visual_encoder_m.blocks.2.mlp.fc1.bias', 'visual_encoder_m.blocks.2.mlp.fc2.weight', 'visual_encoder_m.blocks.2.mlp.fc2.bias', 'visual_encoder_m.blocks.3.norm1.weight', 'visual_encoder_m.blocks.3.norm1.bias', 'visual_encoder_m.blocks.3.attn.qkv.weight', 'visual_encoder_m.blocks.3.attn.qkv.bias', 'visual_encoder_m.blocks.3.attn.proj.weight', 'visual_encoder_m.blocks.3.attn.proj.bias', 'visual_encoder_m.blocks.3.norm2.weight', 'visual_encoder_m.blocks.3.norm2.bias', 'visual_encoder_m.blocks.3.mlp.fc1.weight', 'visual_encoder_m.blocks.3.mlp.fc1.bias', 'visual_encoder_m.blocks.3.mlp.fc2.weight', 'visual_encoder_m.blocks.3.mlp.fc2.bias', 'visual_encoder_m.blocks.4.norm1.weight', 'visual_encoder_m.blocks.4.norm1.bias', 'visual_encoder_m.blocks.4.attn.qkv.weight', 'visual_encoder_m.blocks.4.attn.qkv.bias', 'visual_encoder_m.blocks.4.attn.proj.weight', 'visual_encoder_m.blocks.4.attn.proj.bias', 'visual_encoder_m.blocks.4.norm2.weight', 'visual_encoder_m.blocks.4.norm2.bias', 'visual_encoder_m.blocks.4.mlp.fc1.weight', 'visual_encoder_m.blocks.4.mlp.fc1.bias', 'visual_encoder_m.blocks.4.mlp.fc2.weight', 'visual_encoder_m.blocks.4.mlp.fc2.bias', 'visual_encoder_m.blocks.5.norm1.weight', 'visual_encoder_m.blocks.5.norm1.bias', 'visual_encoder_m.blocks.5.attn.qkv.weight', 'visual_encoder_m.blocks.5.attn.qkv.bias', 'visual_encoder_m.blocks.5.attn.proj.weight', 'visual_encoder_m.blocks.5.attn.proj.bias', 'visual_encoder_m.blocks.5.norm2.weight', 'visual_encoder_m.blocks.5.norm2.bias', 'visual_encoder_m.blocks.5.mlp.fc1.weight', 'visual_encoder_m.blocks.5.mlp.fc1.bias', 'visual_encoder_m.blocks.5.mlp.fc2.weight', 'visual_encoder_m.blocks.5.mlp.fc2.bias', 'visual_encoder_m.blocks.6.norm1.weight', 'visual_encoder_m.blocks.6.norm1.bias', 'visual_encoder_m.blocks.6.attn.qkv.weight', 'visual_encoder_m.blocks.6.attn.qkv.bias', 'visual_encoder_m.blocks.6.attn.proj.weight', 'visual_encoder_m.blocks.6.attn.proj.bias', 'visual_encoder_m.blocks.6.norm2.weight', 'visual_encoder_m.blocks.6.norm2.bias', 'visual_encoder_m.blocks.6.mlp.fc1.weight', 'visual_encoder_m.blocks.6.mlp.fc1.bias', 'visual_encoder_m.blocks.6.mlp.fc2.weight', 'visual_encoder_m.blocks.6.mlp.fc2.bias', 'visual_encoder_m.blocks.7.norm1.weight', 'visual_encoder_m.blocks.7.norm1.bias', 'visual_encoder_m.blocks.7.attn.qkv.weight', 'visual_encoder_m.blocks.7.attn.qkv.bias', 'visual_encoder_m.blocks.7.attn.proj.weight', 'visual_encoder_m.blocks.7.attn.proj.bias', 'visual_encoder_m.blocks.7.norm2.weight', 'visual_encoder_m.blocks.7.norm2.bias', 'visual_encoder_m.blocks.7.mlp.fc1.weight', 'visual_encoder_m.blocks.7.mlp.fc1.bias', 'visual_encoder_m.blocks.7.mlp.fc2.weight', 'visual_encoder_m.blocks.7.mlp.fc2.bias', 'visual_encoder_m.blocks.8.norm1.weight', 'visual_encoder_m.blocks.8.norm1.bias', 'visual_encoder_m.blocks.8.attn.qkv.weight', 'visual_encoder_m.blocks.8.attn.qkv.bias', 'visual_encoder_m.blocks.8.attn.proj.weight', 'visual_encoder_m.blocks.8.attn.proj.bias', 'visual_encoder_m.blocks.8.norm2.weight', 'visual_encoder_m.blocks.8.norm2.bias', 'visual_encoder_m.blocks.8.mlp.fc1.weight', 'visual_encoder_m.blocks.8.mlp.fc1.bias', 'visual_encoder_m.blocks.8.mlp.fc2.weight', 'visual_encoder_m.blocks.8.mlp.fc2.bias', 'visual_encoder_m.blocks.9.norm1.weight', 'visual_encoder_m.blocks.9.norm1.bias', 'visual_encoder_m.blocks.9.attn.qkv.weight', 'visual_encoder_m.blocks.9.attn.qkv.bias', 'visual_encoder_m.blocks.9.attn.proj.weight', 'visual_encoder_m.blocks.9.attn.proj.bias', 'visual_encoder_m.blocks.9.norm2.weight', 'visual_encoder_m.blocks.9.norm2.bias', 'visual_encoder_m.blocks.9.mlp.fc1.weight', 'visual_encoder_m.blocks.9.mlp.fc1.bias', 'visual_encoder_m.blocks.9.mlp.fc2.weight', 'visual_encoder_m.blocks.9.mlp.fc2.bias', 'visual_encoder_m.blocks.10.norm1.weight', 'visual_encoder_m.blocks.10.norm1.bias', 'visual_encoder_m.blocks.10.attn.qkv.weight', 'visual_encoder_m.blocks.10.attn.qkv.bias', 'visual_encoder_m.blocks.10.attn.proj.weight', 'visual_encoder_m.blocks.10.attn.proj.bias', 'visual_encoder_m.blocks.10.norm2.weight', 'visual_encoder_m.blocks.10.norm2.bias', 'visual_encoder_m.blocks.10.mlp.fc1.weight', 'visual_encoder_m.blocks.10.mlp.fc1.bias', 'visual_encoder_m.blocks.10.mlp.fc2.weight', 'visual_encoder_m.blocks.10.mlp.fc2.bias', 'visual_encoder_m.blocks.11.norm1.weight', 'visual_encoder_m.blocks.11.norm1.bias', 'visual_encoder_m.blocks.11.attn.qkv.weight', 'visual_encoder_m.blocks.11.attn.qkv.bias', 'visual_encoder_m.blocks.11.attn.proj.weight', 'visual_encoder_m.blocks.11.attn.proj.bias', 'visual_encoder_m.blocks.11.norm2.weight', 'visual_encoder_m.blocks.11.norm2.bias', 'visual_encoder_m.blocks.11.mlp.fc1.weight', 'visual_encoder_m.blocks.11.mlp.fc1.bias', 'visual_encoder_m.blocks.11.mlp.fc2.weight', 'visual_encoder_m.blocks.11.mlp.fc2.bias', 'visual_encoder_m.norm.weight', 'visual_encoder_m.norm.bias', 'text_encoder_m.embeddings.position_ids', 'text_encoder_m.embeddings.word_embeddings.weight', 'text_encoder_m.embeddings.position_embeddings.weight', 'text_encoder_m.embeddings.token_type_embeddings.weight', 'text_encoder_m.embeddings.LayerNorm.weight', 'text_encoder_m.embeddings.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.encoder.layer.0.output.dense.weight', 'text_encoder_m.encoder.layer.0.output.dense.bias', 'text_encoder_m.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.encoder.layer.1.output.dense.weight', 'text_encoder_m.encoder.layer.1.output.dense.bias', 'text_encoder_m.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.encoder.layer.2.output.dense.weight', 'text_encoder_m.encoder.layer.2.output.dense.bias', 'text_encoder_m.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.encoder.layer.3.output.dense.weight', 'text_encoder_m.encoder.layer.3.output.dense.bias', 'text_encoder_m.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.encoder.layer.4.output.dense.weight', 'text_encoder_m.encoder.layer.4.output.dense.bias', 'text_encoder_m.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.encoder.layer.5.output.dense.weight', 'text_encoder_m.encoder.layer.5.output.dense.bias', 'text_encoder_m.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.attention.self.query.weight', 'text_encoder_m.encoder.layer.6.attention.self.query.bias', 'text_encoder_m.encoder.layer.6.attention.self.key.weight', 'text_encoder_m.encoder.layer.6.attention.self.key.bias', 'text_encoder_m.encoder.layer.6.attention.self.value.weight', 'text_encoder_m.encoder.layer.6.attention.self.value.bias', 'text_encoder_m.encoder.layer.6.attention.output.dense.weight', 'text_encoder_m.encoder.layer.6.attention.output.dense.bias', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.intermediate.dense.weight', 'text_encoder_m.encoder.layer.6.intermediate.dense.bias', 'text_encoder_m.encoder.layer.6.output.dense.weight', 'text_encoder_m.encoder.layer.6.output.dense.bias', 'text_encoder_m.encoder.layer.6.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.attention.self.query.weight', 'text_encoder_m.encoder.layer.7.attention.self.query.bias', 'text_encoder_m.encoder.layer.7.attention.self.key.weight', 'text_encoder_m.encoder.layer.7.attention.self.key.bias', 'text_encoder_m.encoder.layer.7.attention.self.value.weight', 'text_encoder_m.encoder.layer.7.attention.self.value.bias', 'text_encoder_m.encoder.layer.7.attention.output.dense.weight', 'text_encoder_m.encoder.layer.7.attention.output.dense.bias', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.intermediate.dense.weight', 'text_encoder_m.encoder.layer.7.intermediate.dense.bias', 'text_encoder_m.encoder.layer.7.output.dense.weight', 'text_encoder_m.encoder.layer.7.output.dense.bias', 'text_encoder_m.encoder.layer.7.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.attention.self.query.weight', 'text_encoder_m.encoder.layer.8.attention.self.query.bias', 'text_encoder_m.encoder.layer.8.attention.self.key.weight', 'text_encoder_m.encoder.layer.8.attention.self.key.bias', 'text_encoder_m.encoder.layer.8.attention.self.value.weight', 'text_encoder_m.encoder.layer.8.attention.self.value.bias', 'text_encoder_m.encoder.layer.8.attention.output.dense.weight', 'text_encoder_m.encoder.layer.8.attention.output.dense.bias', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.intermediate.dense.weight', 'text_encoder_m.encoder.layer.8.intermediate.dense.bias', 'text_encoder_m.encoder.layer.8.output.dense.weight', 'text_encoder_m.encoder.layer.8.output.dense.bias', 'text_encoder_m.encoder.layer.8.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.attention.self.query.weight', 'text_encoder_m.encoder.layer.9.attention.self.query.bias', 'text_encoder_m.encoder.layer.9.attention.self.key.weight', 'text_encoder_m.encoder.layer.9.attention.self.key.bias', 'text_encoder_m.encoder.layer.9.attention.self.value.weight', 'text_encoder_m.encoder.layer.9.attention.self.value.bias', 'text_encoder_m.encoder.layer.9.attention.output.dense.weight', 'text_encoder_m.encoder.layer.9.attention.output.dense.bias', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.intermediate.dense.weight', 'text_encoder_m.encoder.layer.9.intermediate.dense.bias', 'text_encoder_m.encoder.layer.9.output.dense.weight', 'text_encoder_m.encoder.layer.9.output.dense.bias', 'text_encoder_m.encoder.layer.9.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.attention.self.query.weight', 'text_encoder_m.encoder.layer.10.attention.self.query.bias', 'text_encoder_m.encoder.layer.10.attention.self.key.weight', 'text_encoder_m.encoder.layer.10.attention.self.key.bias', 'text_encoder_m.encoder.layer.10.attention.self.value.weight', 'text_encoder_m.encoder.layer.10.attention.self.value.bias', 'text_encoder_m.encoder.layer.10.attention.output.dense.weight', 'text_encoder_m.encoder.layer.10.attention.output.dense.bias', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.intermediate.dense.weight', 'text_encoder_m.encoder.layer.10.intermediate.dense.bias', 'text_encoder_m.encoder.layer.10.output.dense.weight', 'text_encoder_m.encoder.layer.10.output.dense.bias', 'text_encoder_m.encoder.layer.10.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.attention.self.query.weight', 'text_encoder_m.encoder.layer.11.attention.self.query.bias', 'text_encoder_m.encoder.layer.11.attention.self.key.weight', 'text_encoder_m.encoder.layer.11.attention.self.key.bias', 'text_encoder_m.encoder.layer.11.attention.self.value.weight', 'text_encoder_m.encoder.layer.11.attention.self.value.bias', 'text_encoder_m.encoder.layer.11.attention.output.dense.weight', 'text_encoder_m.encoder.layer.11.attention.output.dense.bias', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.intermediate.dense.weight', 'text_encoder_m.encoder.layer.11.intermediate.dense.bias', 'text_encoder_m.encoder.layer.11.output.dense.weight', 'text_encoder_m.encoder.layer.11.output.dense.bias', 'text_encoder_m.encoder.layer.11.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.output.LayerNorm.bias', 'text_decoder_m.bert.embeddings.position_ids', 'text_decoder_m.bert.embeddings.word_embeddings.weight', 'text_decoder_m.bert.embeddings.position_embeddings.weight', 'text_decoder_m.bert.embeddings.token_type_embeddings.weight', 'text_decoder_m.bert.embeddings.LayerNorm.weight', 'text_decoder_m.bert.embeddings.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.0.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.0.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.0.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.0.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.0.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.0.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.0.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.0.output.dense.weight', 'text_decoder_m.bert.encoder.layer.0.output.dense.bias', 'text_decoder_m.bert.encoder.layer.0.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.0.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.1.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.1.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.1.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.1.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.1.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.1.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.1.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.1.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.1.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.1.output.dense.weight', 'text_decoder_m.bert.encoder.layer.1.output.dense.bias', 'text_decoder_m.bert.encoder.layer.1.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.1.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.2.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.2.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.2.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.2.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.2.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.2.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.2.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.2.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.2.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.2.output.dense.weight', 'text_decoder_m.bert.encoder.layer.2.output.dense.bias', 'text_decoder_m.bert.encoder.layer.2.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.2.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.3.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.3.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.3.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.3.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.3.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.3.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.3.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.3.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.3.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.3.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.3.output.dense.weight', 'text_decoder_m.bert.encoder.layer.3.output.dense.bias', 'text_decoder_m.bert.encoder.layer.3.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.3.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.4.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.4.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.4.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.4.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.4.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.4.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.4.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.4.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.4.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.4.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.4.output.dense.weight', 'text_decoder_m.bert.encoder.layer.4.output.dense.bias', 'text_decoder_m.bert.encoder.layer.4.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.4.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.5.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.5.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.5.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.5.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.5.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.5.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.5.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.5.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.5.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.5.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.5.output.dense.weight', 'text_decoder_m.bert.encoder.layer.5.output.dense.bias', 'text_decoder_m.bert.encoder.layer.5.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.5.output.LayerNorm.bias', 'text_decoder_m.cls.predictions.bias', 'text_decoder_m.cls.predictions.transform.dense.weight', 'text_decoder_m.cls.predictions.transform.dense.bias', 'text_decoder_m.cls.predictions.transform.LayerNorm.weight', 'text_decoder_m.cls.predictions.transform.LayerNorm.bias', 'text_decoder_m.cls.predictions.decoder.weight', 'text_decoder_m.cls.predictions.decoder.bias'], unexpected_keys=['model', 'optimizer', 'lr_scheduler', 'config', 'epoch'])\n"
          ]
        }
      ],
      "source": [
        "# load check point to continue training\n",
        "if args.checkpoint:\n",
        "    checkpoint = torch.load(args.checkpoint, map_location='cpu')\n",
        "    if args.evaluate:\n",
        "        state_dict = checkpoint\n",
        "    else:\n",
        "        state_dict = checkpoint['model']\n",
        "\n",
        "    # with checkpoint of vqa model\n",
        "    # reshape positional embedding to accomodate for image resolution change\n",
        "    # pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)\n",
        "    # state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
        "\n",
        "    # Check if the key exists before accessing it\n",
        "    if 'visual_encoder.pos_embed' in state_dict:\n",
        "        # reshape positional embedding to accomodate for image resolution change\n",
        "        pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)\n",
        "        state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
        "    else:\n",
        "        print(\"Warning: 'visual_encoder.pos_embed' not found in checkpoint. Skipping positional embedding interpolation.\")\n",
        "\n",
        "\n",
        "    if not args.evaluate:\n",
        "        if config['distill']:\n",
        "            m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)\n",
        "            state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped\n",
        "\n",
        "        for key in list(state_dict.keys()):\n",
        "            if 'bert' in key:\n",
        "                encoder_key = key.replace('bert.','')\n",
        "                state_dict[encoder_key] = state_dict[key]\n",
        "            # intialize text decoder as multimodal encoder (last 6 layers of model.text_encoder)\n",
        "            if 'text_encoder' in key:\n",
        "                if 'layer' in key:\n",
        "                    # print(key)\n",
        "                    encoder_keys = key.split('.')\n",
        "                    print(encoder_keys)\n",
        "                    # print(encoder_keys[4])\n",
        "                    tmp_fix_idx = 4 # for the downsized model, idx 5 is the layer number\n",
        "                    layer_num = int(encoder_keys[tmp_fix_idx]) # 4\n",
        "                    if layer_num<6:\n",
        "                        del state_dict[key]\n",
        "                        continue\n",
        "                    else:\n",
        "                        decoder_layer_num = (layer_num-6)\n",
        "                        encoder_keys[4] = str(decoder_layer_num)\n",
        "                        encoder_key = '.'.join(encoder_keys)\n",
        "                else:\n",
        "                    encoder_key = key\n",
        "                decoder_key = encoder_key.replace('text_encoder','text_decoder')\n",
        "                state_dict[decoder_key] = state_dict[key]\n",
        "\n",
        "                del state_dict[key]\n",
        "\n",
        "    msg = model.load_state_dict(state_dict,strict=False)\n",
        "    print('load checkpoint from %s'%args.checkpoint)\n",
        "    print(msg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VJmSxRhtFajJ"
      },
      "outputs": [],
      "source": [
        "# handle distributed training\n",
        "model_without_ddp = model\n",
        "if args.distributed:\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "    model_without_ddp = model.module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "id": "VyQyvZE1FajJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507de1fd-58d2-4420-f170-03ca9cce6350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training\n",
            "Generate VQA test result:  [   0/6997]  eta: 6:29:33    time: 3.3404  data: 1.7344  max mem: 26303\n",
            "Generate VQA test result:  [  50/6997]  eta: 2:28:27    time: 1.2431  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 100/6997]  eta: 2:25:03    time: 1.2435  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 150/6997]  eta: 2:22:57    time: 1.2326  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 200/6997]  eta: 2:21:29    time: 1.2374  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 250/6997]  eta: 2:20:09    time: 1.2385  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 300/6997]  eta: 2:18:57    time: 1.2456  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 350/6997]  eta: 2:17:45    time: 1.2277  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 400/6997]  eta: 2:16:36    time: 1.2384  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 450/6997]  eta: 2:15:34    time: 1.2380  data: 0.0004  max mem: 26424\n",
            "Generate VQA test result:  [ 500/6997]  eta: 2:14:30    time: 1.2441  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 550/6997]  eta: 2:13:25    time: 1.2369  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 600/6997]  eta: 2:12:20    time: 1.2383  data: 0.0002  max mem: 26424\n",
            "Generate VQA test result:  [ 650/6997]  eta: 2:11:19    time: 1.2414  data: 0.0003  max mem: 26424\n",
            "Generate VQA test result:  [ 700/6997]  eta: 2:10:17    time: 1.2403  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [ 750/6997]  eta: 2:09:13    time: 1.2371  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [ 800/6997]  eta: 2:08:09    time: 1.2353  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [ 850/6997]  eta: 2:07:08    time: 1.2482  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [ 900/6997]  eta: 2:06:05    time: 1.2388  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [ 950/6997]  eta: 2:05:02    time: 1.2442  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1000/6997]  eta: 2:04:01    time: 1.2379  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1050/6997]  eta: 2:02:59    time: 1.2432  data: 0.0003  max mem: 26449\n",
            "Generate VQA test result:  [1100/6997]  eta: 2:01:56    time: 1.2401  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1150/6997]  eta: 2:00:55    time: 1.2440  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1200/6997]  eta: 1:59:53    time: 1.2390  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1250/6997]  eta: 1:58:50    time: 1.2405  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1300/6997]  eta: 1:57:47    time: 1.2333  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1350/6997]  eta: 1:56:45    time: 1.2411  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1400/6997]  eta: 1:55:42    time: 1.2360  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1450/6997]  eta: 1:54:40    time: 1.2367  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1500/6997]  eta: 1:53:37    time: 1.2372  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1550/6997]  eta: 1:52:35    time: 1.2446  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1600/6997]  eta: 1:51:33    time: 1.2380  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1650/6997]  eta: 1:50:30    time: 1.2346  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1700/6997]  eta: 1:49:28    time: 1.2449  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1750/6997]  eta: 1:48:27    time: 1.2427  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1800/6997]  eta: 1:47:25    time: 1.2463  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1850/6997]  eta: 1:46:23    time: 1.2440  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1900/6997]  eta: 1:45:21    time: 1.2464  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [1950/6997]  eta: 1:44:19    time: 1.2364  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2000/6997]  eta: 1:43:17    time: 1.2401  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2050/6997]  eta: 1:42:15    time: 1.2486  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2100/6997]  eta: 1:41:13    time: 1.2381  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2150/6997]  eta: 1:40:12    time: 1.2505  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2200/6997]  eta: 1:39:13    time: 1.3232  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2250/6997]  eta: 1:38:11    time: 1.2404  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2300/6997]  eta: 1:37:09    time: 1.2362  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2350/6997]  eta: 1:36:06    time: 1.2428  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2400/6997]  eta: 1:35:04    time: 1.2402  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2450/6997]  eta: 1:34:02    time: 1.2392  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2500/6997]  eta: 1:33:00    time: 1.2391  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2550/6997]  eta: 1:31:57    time: 1.2384  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2600/6997]  eta: 1:30:55    time: 1.2438  data: 0.0002  max mem: 26449\n",
            "Generate VQA test result:  [2650/6997]  eta: 1:29:53    time: 1.2369  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [2700/6997]  eta: 1:28:51    time: 1.2385  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [2750/6997]  eta: 1:27:48    time: 1.2408  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [2800/6997]  eta: 1:26:46    time: 1.2347  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [2850/6997]  eta: 1:25:44    time: 1.2429  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [2900/6997]  eta: 1:24:42    time: 1.2435  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [2950/6997]  eta: 1:23:41    time: 1.2483  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3000/6997]  eta: 1:22:38    time: 1.2367  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3050/6997]  eta: 1:21:36    time: 1.2445  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3100/6997]  eta: 1:20:34    time: 1.2358  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3150/6997]  eta: 1:19:32    time: 1.2361  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3200/6997]  eta: 1:18:30    time: 1.2406  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3250/6997]  eta: 1:17:28    time: 1.2431  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3300/6997]  eta: 1:16:26    time: 1.2351  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3350/6997]  eta: 1:15:23    time: 1.2370  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3400/6997]  eta: 1:14:21    time: 1.2348  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3450/6997]  eta: 1:13:19    time: 1.2461  data: 0.0003  max mem: 26498\n",
            "Generate VQA test result:  [3500/6997]  eta: 1:12:17    time: 1.2371  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3550/6997]  eta: 1:11:14    time: 1.2325  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3600/6997]  eta: 1:10:12    time: 1.2382  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3650/6997]  eta: 1:09:10    time: 1.2368  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3700/6997]  eta: 1:08:08    time: 1.2372  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3750/6997]  eta: 1:07:06    time: 1.2397  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3800/6997]  eta: 1:06:04    time: 1.2370  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3850/6997]  eta: 1:05:02    time: 1.2352  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3900/6997]  eta: 1:04:00    time: 1.2342  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [3950/6997]  eta: 1:02:58    time: 1.2381  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4000/6997]  eta: 1:01:56    time: 1.2537  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4050/6997]  eta: 1:00:54    time: 1.2363  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4100/6997]  eta: 0:59:52    time: 1.2321  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4150/6997]  eta: 0:58:50    time: 1.2317  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4200/6997]  eta: 0:57:48    time: 1.2328  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4250/6997]  eta: 0:56:45    time: 1.2330  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4300/6997]  eta: 0:55:43    time: 1.2395  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4350/6997]  eta: 0:54:41    time: 1.2409  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4400/6997]  eta: 0:53:39    time: 1.2399  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4450/6997]  eta: 0:52:37    time: 1.2472  data: 0.0003  max mem: 26498\n",
            "Generate VQA test result:  [4500/6997]  eta: 0:51:35    time: 1.2316  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4550/6997]  eta: 0:50:33    time: 1.2393  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4600/6997]  eta: 0:49:31    time: 1.2414  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4650/6997]  eta: 0:48:29    time: 1.2363  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4700/6997]  eta: 0:47:27    time: 1.2353  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4750/6997]  eta: 0:46:25    time: 1.2397  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4800/6997]  eta: 0:45:23    time: 1.2358  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4850/6997]  eta: 0:44:21    time: 1.2354  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4900/6997]  eta: 0:43:19    time: 1.2385  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [4950/6997]  eta: 0:42:17    time: 1.2440  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5000/6997]  eta: 0:41:15    time: 1.2355  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5050/6997]  eta: 0:40:13    time: 1.2365  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5100/6997]  eta: 0:39:11    time: 1.2443  data: 0.0003  max mem: 26498\n",
            "Generate VQA test result:  [5150/6997]  eta: 0:38:09    time: 1.2349  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5200/6997]  eta: 0:37:07    time: 1.2420  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5250/6997]  eta: 0:36:05    time: 1.2320  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5300/6997]  eta: 0:35:03    time: 1.2406  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5350/6997]  eta: 0:34:01    time: 1.2403  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5400/6997]  eta: 0:32:59    time: 1.2466  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5450/6997]  eta: 0:31:57    time: 1.2447  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5500/6997]  eta: 0:30:55    time: 1.2466  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5550/6997]  eta: 0:29:53    time: 1.2454  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5600/6997]  eta: 0:28:51    time: 1.2351  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5650/6997]  eta: 0:27:49    time: 1.2342  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5700/6997]  eta: 0:26:47    time: 1.2421  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5750/6997]  eta: 0:25:45    time: 1.2405  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5800/6997]  eta: 0:24:43    time: 1.2411  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5850/6997]  eta: 0:23:41    time: 1.2351  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5900/6997]  eta: 0:22:39    time: 1.2359  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [5950/6997]  eta: 0:21:37    time: 1.2382  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6000/6997]  eta: 0:20:35    time: 1.2395  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6050/6997]  eta: 0:19:33    time: 1.2382  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6100/6997]  eta: 0:18:31    time: 1.2438  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6150/6997]  eta: 0:17:29    time: 1.2340  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6200/6997]  eta: 0:16:27    time: 1.2427  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6250/6997]  eta: 0:15:25    time: 1.2358  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6300/6997]  eta: 0:14:23    time: 1.2430  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6350/6997]  eta: 0:13:21    time: 1.2419  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6400/6997]  eta: 0:12:19    time: 1.2355  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6450/6997]  eta: 0:11:18    time: 1.2429  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6500/6997]  eta: 0:10:16    time: 1.2372  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6550/6997]  eta: 0:09:14    time: 1.2345  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6600/6997]  eta: 0:08:12    time: 1.2363  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6650/6997]  eta: 0:07:10    time: 1.2415  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6700/6997]  eta: 0:06:08    time: 1.2447  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6750/6997]  eta: 0:05:06    time: 1.2440  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6800/6997]  eta: 0:04:04    time: 1.2371  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6850/6997]  eta: 0:03:02    time: 1.2442  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6900/6997]  eta: 0:02:00    time: 1.2515  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6950/6997]  eta: 0:00:58    time: 1.2444  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result:  [6996/6997]  eta: 0:00:01    time: 1.2428  data: 0.0002  max mem: 26498\n",
            "Generate VQA test result: Total time: 2:24:33 (1.2397 s / it)\n",
            "result file saved to output/vqa_PretrainModel_freezeTune/result/vqa_result_epoch0.json\n",
            "Time time 2:24:36\n"
          ]
        }
      ],
      "source": [
        "# run training single GPU\n",
        "print(\"Start training\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, max_epoch):\n",
        "    if epoch>0:\n",
        "        lr_scheduler.step(epoch+warmup_steps)\n",
        "\n",
        "    if not args.evaluate:\n",
        "        if args.distributed:\n",
        "            train_loader.sampler.set_epoch(epoch)\n",
        "\n",
        "        train_stats = train(model, train_loader, optimizer, tokenizer, epoch, warmup_steps, device, lr_scheduler, config)\n",
        "\n",
        "    if args.evaluate:\n",
        "        break\n",
        "\n",
        "    if utils.is_main_process():\n",
        "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                      'epoch': epoch,\n",
        "                    }\n",
        "        with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
        "            f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "        save_obj = {\n",
        "            'model': model_without_ddp.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'lr_scheduler': lr_scheduler.state_dict(),\n",
        "            'config': config,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_%02d.pth'%epoch))\n",
        "\n",
        "        # save result to google drive\n",
        "        !cp -r {args.output_dir} {GOOGLE_DRIVE_PATH}\n",
        "\n",
        "    if args.distributed:\n",
        "        dist.barrier()\n",
        "    else:\n",
        "        pass  # Skip barrier for non-distributed training\n",
        "\n",
        "# evaluation\n",
        "vqa_result = evaluation(model, test_loader, tokenizer, device, config)\n",
        "result_file = save_result(vqa_result, args.result_dir, 'vqa_result_epoch%d'%epoch)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print('Time time {}'.format(total_time_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "T52hIhCtaoJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d756842-baba-4de0-ecdb-dd2cf9e0476b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google drive: /content/drive/MyDrive/DL_Project/ALBEF from colab drive: output/vqa_PretrainModel_freezeTune\n"
          ]
        }
      ],
      "source": [
        "print(f'google drive: {GOOGLE_DRIVE_PATH} from colab drive: {args.output_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qmVYMgYzgka2"
      },
      "outputs": [],
      "source": [
        "# save result to google drive\n",
        "!cp -r {args.output_dir} {GOOGLE_DRIVE_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9o33GkkgsBGD"
      },
      "outputs": [],
      "source": [
        "# terminate colab runtime\n",
        "from google.colab import runtime\n",
        "runtime.unassign()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}