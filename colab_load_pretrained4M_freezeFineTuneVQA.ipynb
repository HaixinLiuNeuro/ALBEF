{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HaixinLiuNeuro/ALBEF/blob/main/colab_load_pretrained4M_noFineTuen_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24nSaNVhFajG"
   },
   "source": [
    "# Load pretrained 4M model, without fine-tune with only VQA dataset, run evaluation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpeIswepIcEs"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-C2cjy7sLsaa"
   },
   "outputs": [],
   "source": [
    "# setup drive folder\n",
    "import os\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you want to save result\n",
    "GOOGLE_DRIVE_PATH_POST_MYDRIVE = os.path.join('DL_Project', 'ALBEF')\n",
    "GOOGLE_DRIVE_PATH = os.path.join('/content', 'drive', 'MyDrive', GOOGLE_DRIVE_PATH_POST_MYDRIVE)\n",
    "os.makedirs(GOOGLE_DRIVE_PATH, exist_ok=True)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gnMy6WHMjZL"
   },
   "outputs": [],
   "source": [
    "# if running locally set GOOGLE PATH\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  print(f'Running in google colab. Our path is `{GOOGLE_DRIVE_PATH}`')\n",
    "else:\n",
    "  GOOGLE_DRIVE_PATH = '.'\n",
    "  print('Running locally.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vaWO3M8Mmxi"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "print(f'Google Drive Path: {GOOGLE_DRIVE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cj6rXs28G5ck"
   },
   "outputs": [],
   "source": [
    "# Clone the repo to a content\n",
    "!git clone -b main https://github.com/HaixinLiuNeuro/ALBEF.git /tmp/ALBEF\n",
    "!cp -r /tmp/ALBEF/* .\n",
    "!rm -rf /tmp/ALBEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "N_7q2p11MyGc"
   },
   "outputs": [],
   "source": [
    "# install dependency\n",
    "!pip install transformers==4.25.1\n",
    "!pip install ruamel.yaml==0.17.*\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQxFlSxfFajH"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "\n",
    "# use vqa model\n",
    "from models.model_vqa_freeze import ALBEF\n",
    "\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "\n",
    "import utils\n",
    "from dataset.utils import save_result\n",
    "from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn\n",
    "\n",
    "from scheduler import create_scheduler\n",
    "from optim import create_optimizer\n",
    "\n",
    "# print and plotting\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiebcG8heA07"
   },
   "outputs": [],
   "source": [
    "# %reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPxGkhRy4FLD"
   },
   "outputs": [],
   "source": [
    "# prep data\n",
    "# download from website\n",
    "\n",
    "# make folder /content/data\n",
    "DATA_PATH = os.path.join('/content', 'data')\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "%cd /content/data\n",
    "\n",
    "# download data from links:\n",
    "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/json_pretrain.zip\n",
    "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/data.tar.gz\n",
    "# http://images.cocodataset.org/zips/train2014.zip\n",
    "# http://images.cocodataset.org/zips/val2014.zip\n",
    "# http://images.cocodataset.org/zips/test2015.zip\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the download links\n",
    "links = [\n",
    "    # \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/json_pretrain.zip\", # pretrain json\n",
    "    \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/data.tar.gz\", # for downstream task json\n",
    "    \"http://images.cocodataset.org/zips/train2014.zip\", # comment out if only run evaluation\n",
    "    \"http://images.cocodataset.org/zips/val2014.zip\",   # comment out if only run evaluation\n",
    "    \"http://images.cocodataset.org/zips/test2015.zip\"\n",
    "]\n",
    "\n",
    "# Download and extract each file\n",
    "for link in links:\n",
    "    filename = link.split('/')[-1]\n",
    "    print(f\"Downloading {filename}...\")\n",
    "\n",
    "    # Download file\n",
    "    !wget -q --show-progress {link}\n",
    "\n",
    "    print(f\"Extracting {filename}...\")\n",
    "\n",
    "    # Extract based on file extension\n",
    "    if filename.endswith('.zip'):\n",
    "      if '//images.cocodataset.org/zips/' in link:\n",
    "        !unzip -q {filename}\n",
    "      else:\n",
    "        !unzip -q -j {filename}  # -j option flattens the directory structure for json_pretrain.zip\n",
    "    elif filename.endswith('.tar.gz'):\n",
    "        !tar -xzf {filename} --strip-components=1  # Remove the top-level directory\n",
    "\n",
    "    # Delete the zip/tar file after extraction\n",
    "    print(f\"Removing {filename}...\")\n",
    "    !rm {filename}\n",
    "\n",
    "    print(f\"Finished processing {filename}\")\n",
    "\n",
    "print(\"All downloads and extractions completed!\")\n",
    "\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sc9XbDXyL4WJ"
   },
   "outputs": [],
   "source": [
    "# !rm -rf /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9cbgfXjUT_t"
   },
   "outputs": [],
   "source": [
    "# check files\n",
    "%cd /content/data\n",
    "!ls\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWKM1ISxSrPc"
   },
   "outputs": [],
   "source": [
    "#\n",
    "FETCH_PRETRAINED_MODEL = True\n",
    "%cd /content\n",
    "\n",
    "# download data from links:\n",
    "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\n",
    "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/vqa.pth\n",
    "# model check point from training\n",
    "# https://drive.google.com/file/d/1yEsyeB0FkIgWlT2Way_KFLPNLCQy6KoU/view?usp=sharing\n",
    "\n",
    "if FETCH_PRETRAINED_MODEL:\n",
    "\n",
    "  # Define the download links\n",
    "  links = [\n",
    "      \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\",\n",
    "      # \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/vqa.pth\"\n",
    "  ]\n",
    "\n",
    "  # Download and extract each file\n",
    "  for link in links:\n",
    "      filename = link.split('/')[-1]\n",
    "      print(f\"Downloading {filename}...\")\n",
    "\n",
    "      # Download file\n",
    "      !wget -q --show-progress {link}\n",
    "\n",
    "\n",
    "      print(f\"Finished processing {filename}\")\n",
    "\n",
    "  print(\"All model downloads completed!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm5adbe9FajI"
   },
   "source": [
    "## Setup for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SSGJE2MFajI"
   },
   "outputs": [],
   "source": [
    "# config\n",
    "%cd /content\n",
    "args = argparse.Namespace()\n",
    "args.config = './configs/VQA.yaml'\n",
    "args.checkpoint = './ALBEF_4M.pth'\n",
    "args.output_dir = 'output/vqa_PretrainModel_freezeTune'\n",
    "args.evaluate = True # to train use False\n",
    "args.text_encoder = 'bert-base-uncased'\n",
    "args.text_decoder = 'bert-base-uncased'\n",
    "args.device = 'cuda'\n",
    "args.seed = 42\n",
    "args.distributed = False\n",
    "\n",
    "config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n",
    "pprint(config)\n",
    "\n",
    "# make result folder and save config\n",
    "args.result_dir = os.path.join(args.output_dir, 'result')\n",
    "\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyGrj0NZFajI"
   },
   "outputs": [],
   "source": [
    "# training functions\n",
    "def train(model, data_loader, optimizer, tokenizer, epoch, warmup_steps, device, scheduler, config):\n",
    "    # train\n",
    "    model.train()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    metric_logger.add_meter('loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
    "\n",
    "    header = 'Train Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 50\n",
    "    step_size = 100\n",
    "    warmup_iterations = warmup_steps*step_size\n",
    "\n",
    "    for i,(image, question, answer, weights, n) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        image, weights = image.to(device,non_blocking=True), weights.to(device,non_blocking=True)\n",
    "        question_input = tokenizer(question, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\").to(device)\n",
    "        answer_input = tokenizer(answer, padding='longest', return_tensors=\"pt\").to(device)\n",
    "\n",
    "        if epoch>0 or not config['warm_up']:\n",
    "            alpha = config['alpha']\n",
    "        else:\n",
    "            alpha = config['alpha']*min(1,i/len(data_loader))\n",
    "\n",
    "        loss = model(image, question_input, answer_input, train=True, alpha=alpha, k=n, weights=weights)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        if epoch==0 and i%step_size==0 and i<=warmup_iterations:\n",
    "            scheduler.step(i//step_size)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger.global_avg())\n",
    "    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluation(model, data_loader, tokenizer, device, config) :\n",
    "    # test\n",
    "    model.eval()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Generate VQA test result:'\n",
    "    print_freq = 50\n",
    "\n",
    "    result = []\n",
    "\n",
    "    answer_list = [answer+config['eos'] for answer in data_loader.dataset.answer_list]\n",
    "    answer_input = tokenizer(answer_list, padding='longest', return_tensors='pt').to(device)\n",
    "\n",
    "    for n, (image, question, question_id) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        image = image.to(device,non_blocking=True)\n",
    "        question_input = tokenizer(question, padding='longest', return_tensors=\"pt\").to(device)\n",
    "\n",
    "        topk_ids, topk_probs = model(image, question_input, answer_input, train=False, k=config['k_test'])\n",
    "\n",
    "        for ques_id, topk_id, topk_prob in zip(question_id, topk_ids, topk_probs):\n",
    "            ques_id = int(ques_id.item())\n",
    "            _, pred = topk_prob.max(dim=0)\n",
    "            result.append({\"question_id\":ques_id, \"answer\":data_loader.dataset.answer_list[topk_id[pred]]})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_fo-oPwFajI"
   },
   "outputs": [],
   "source": [
    "# setup for training/evaluation (from main)\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "print(f'device: {device}')\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "start_epoch = 0\n",
    "max_epoch = config['schedular']['epochs']\n",
    "warmup_steps = config['schedular']['warmup_epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmtKe-XVFajI"
   },
   "outputs": [],
   "source": [
    "# make dataset and dataloader\n",
    "print(\"Creating vqa datasets\")\n",
    "datasets = create_dataset('vqa', config)\n",
    "\n",
    "if args.distributed:\n",
    "    num_tasks = utils.get_world_size()\n",
    "    global_rank = utils.get_rank()\n",
    "    samplers = create_sampler(datasets, [True, False], num_tasks, global_rank)\n",
    "else:\n",
    "    samplers = [None, None]\n",
    "\n",
    "train_loader, test_loader = create_loader(datasets,samplers,\n",
    "                                          batch_size=[config['batch_size_train'],config['batch_size_test']],\n",
    "                                          num_workers=[4,4],is_trains=[True, False],\n",
    "                                          collate_fns=[vqa_collate_fn,None])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zYcjMhhFajI"
   },
   "outputs": [],
   "source": [
    "#### Model ####\n",
    "print(\"Creating model\")\n",
    "model = ALBEF(config=config, text_encoder=args.text_encoder, text_decoder=args.text_decoder, tokenizer=tokenizer)\n",
    "model = model.to(device)\n",
    "\n",
    "arg_opt = utils.AttrDict(config['optimizer'])\n",
    "optimizer = create_optimizer(arg_opt, model)\n",
    "arg_sche = utils.AttrDict(config['schedular'])\n",
    "lr_scheduler, _ = create_scheduler(arg_sche, optimizer)\n",
    "\n",
    "# check model\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B64EarwbFajJ"
   },
   "outputs": [],
   "source": [
    "# load check point to continue training\n",
    "if args.checkpoint:\n",
    "    checkpoint = torch.load(args.checkpoint, map_location='cpu')\n",
    "    if args.evaluate:\n",
    "        state_dict = checkpoint\n",
    "    else:\n",
    "        state_dict = checkpoint['model']\n",
    "\n",
    "    # with checkpoint of vqa model\n",
    "    # reshape positional embedding to accomodate for image resolution change\n",
    "    # pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)\n",
    "    # state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "\n",
    "    # Check if the key exists before accessing it\n",
    "    if 'visual_encoder.pos_embed' in state_dict:\n",
    "        # reshape positional embedding to accomodate for image resolution change\n",
    "        pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)\n",
    "        state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "    else:\n",
    "        print(\"Warning: 'visual_encoder.pos_embed' not found in checkpoint. Skipping positional embedding interpolation.\")\n",
    "\n",
    "\n",
    "    if not args.evaluate:\n",
    "        if config['distill']:\n",
    "            m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)\n",
    "            state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped\n",
    "\n",
    "        for key in list(state_dict.keys()):\n",
    "            if 'bert' in key:\n",
    "                encoder_key = key.replace('bert.','')\n",
    "                state_dict[encoder_key] = state_dict[key]\n",
    "            # intialize text decoder as multimodal encoder (last 6 layers of model.text_encoder)\n",
    "            if 'text_encoder' in key:\n",
    "                if 'layer' in key:\n",
    "                    # print(key)\n",
    "                    encoder_keys = key.split('.')\n",
    "                    print(encoder_keys)\n",
    "                    # print(encoder_keys[4])\n",
    "                    tmp_fix_idx = 4 # for the downsized model, idx 5 is the layer number\n",
    "                    layer_num = int(encoder_keys[tmp_fix_idx]) # 4\n",
    "                    if layer_num<6:\n",
    "                        del state_dict[key]\n",
    "                        continue\n",
    "                    else:\n",
    "                        decoder_layer_num = (layer_num-6)\n",
    "                        encoder_keys[4] = str(decoder_layer_num)\n",
    "                        encoder_key = '.'.join(encoder_keys)\n",
    "                else:\n",
    "                    encoder_key = key\n",
    "                decoder_key = encoder_key.replace('text_encoder','text_decoder')\n",
    "                state_dict[decoder_key] = state_dict[key]\n",
    "\n",
    "                del state_dict[key]\n",
    "\n",
    "    msg = model.load_state_dict(state_dict,strict=False)\n",
    "    print('load checkpoint from %s'%args.checkpoint)\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJmSxRhtFajJ"
   },
   "outputs": [],
   "source": [
    "# handle distributed training\n",
    "model_without_ddp = model\n",
    "if args.distributed:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "    model_without_ddp = model.module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "VyQyvZE1FajJ"
   },
   "outputs": [],
   "source": [
    "# run evaluation without training single GPU\n",
    "print(\"Start eval\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, max_epoch):\n",
    "    if epoch>0:\n",
    "        lr_scheduler.step(epoch+warmup_steps)\n",
    "\n",
    "    if not args.evaluate:\n",
    "        if args.distributed:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        train_stats = train(model, train_loader, optimizer, tokenizer, epoch, warmup_steps, device, lr_scheduler, config)\n",
    "\n",
    "    if args.evaluate:\n",
    "        break\n",
    "\n",
    "    if utils.is_main_process():\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                      'epoch': epoch,\n",
    "                    }\n",
    "        with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "        save_obj = {\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'config': config,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_%02d.pth'%epoch))\n",
    "    if args.distributed:\n",
    "        dist.barrier()\n",
    "    else:\n",
    "        pass  # Skip barrier for non-distributed training\n",
    "\n",
    "# evaluation\n",
    "vqa_result = evaluation(model, test_loader, tokenizer, device, config)\n",
    "result_file = save_result(vqa_result, args.result_dir, 'vqa_result_epoch%d'%epoch)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Time time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T52hIhCtaoJ0"
   },
   "outputs": [],
   "source": [
    "print(f'google drive: {GOOGLE_DRIVE_PATH} from colab drive: {args.output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmVYMgYzgka2"
   },
   "outputs": [],
   "source": [
    "# save result to google drive\n",
    "!cp -r {args.output_dir} {GOOGLE_DRIVE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o33GkkgsBGD"
   },
   "outputs": [],
   "source": [
    "# terminate colab runtime\n",
    "from google.colab import runtime\n",
    "runtime.unassign()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
