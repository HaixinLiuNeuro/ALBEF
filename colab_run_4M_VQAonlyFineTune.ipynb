{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaixinLiuNeuro/ALBEF/blob/main/colab_run_4M_VQAonlyFineTune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24nSaNVhFajG"
      },
      "source": [
        "# Load pretrained model, fine-tune with only VQA dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "jpeIswepIcEs",
        "outputId": "433b8ed7-b465-4522-dd9d-5b8b4656e4e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup drive folder\n",
        "import os\n",
        "\n",
        "# TODO: Fill in the Google Drive path where you want to save result\n",
        "GOOGLE_DRIVE_PATH_POST_MYDRIVE = os.path.join('DL_Project', 'ALBEF')\n",
        "GOOGLE_DRIVE_PATH = os.path.join('/content', 'drive', 'MyDrive', GOOGLE_DRIVE_PATH_POST_MYDRIVE)\n",
        "os.makedirs(GOOGLE_DRIVE_PATH, exist_ok=True)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ],
      "metadata": {
        "id": "-C2cjy7sLsaa",
        "outputId": "c1c8ed04-046b-446b-a87e-b78fbef72789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if running locally set GOOGLE PATH\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  print(f'Running in google colab. Our path is `{GOOGLE_DRIVE_PATH}`')\n",
        "else:\n",
        "  GOOGLE_DRIVE_PATH = '.'\n",
        "  print('Running locally.')"
      ],
      "metadata": {
        "id": "3gnMy6WHMjZL",
        "outputId": "35540a7c-2266-4670-f63e-962b994f2780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in google colab. Our path is `/content/drive/MyDrive/DL_Project/ALBEF`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import math\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "print(f'Google Drive Path: {GOOGLE_DRIVE_PATH}')"
      ],
      "metadata": {
        "id": "7vaWO3M8Mmxi",
        "outputId": "a6309420-e314-4170-acc9-ea34b09606b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive Path: /content/drive/MyDrive/DL_Project/ALBEF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repo to a content\n",
        "!git clone -b main https://github.com/HaixinLiuNeuro/ALBEF.git /tmp/ALBEF\n",
        "!cp -r /tmp/ALBEF/* .\n",
        "!rm -rf /tmp/ALBEF"
      ],
      "metadata": {
        "id": "cj6rXs28G5ck",
        "outputId": "5667a671-7ca1-491c-9005-13d5f94562a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/tmp/ALBEF'...\n",
            "remote: Enumerating objects: 362, done.\u001b[K\n",
            "remote: Counting objects: 100% (217/217), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 362 (delta 118), reused 111 (delta 108), pack-reused 145 (from 1)\u001b[K\n",
            "Receiving objects: 100% (362/362), 71.57 MiB | 19.97 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependency\n",
        "!pip install transformers==4.25.1\n",
        "!pip install ruamel.yaml==0.17.*\n",
        "!pip install matplotlib\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N_7q2p11MyGc",
        "outputId": "5bb98710-1a67-4115-8615-561247885e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.25.1\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl.metadata (93 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.25.1)\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.25.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.25.1) (2025.1.31)\n",
            "Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.25.1\n",
            "Collecting ruamel.yaml==0.17.*\n",
            "  Downloading ruamel.yaml-0.17.40-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml==0.17.*)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading ruamel.yaml-0.17.40-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml\n",
            "Successfully installed ruamel.yaml-0.17.40 ruamel.yaml.clib-0.2.12\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nQxFlSxfFajH",
        "outputId": "6cfa984e-c182-401e-9c6b-c81f77e72462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# import\n",
        "import argparse\n",
        "import os\n",
        "import ruamel.yaml as yaml\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "\n",
        "# use vqa model\n",
        "from models.model_vqa import ALBEF\n",
        "\n",
        "from models.vit import interpolate_pos_embed\n",
        "from models.tokenization_bert import BertTokenizer\n",
        "\n",
        "import utils\n",
        "from dataset.utils import save_result\n",
        "from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn\n",
        "\n",
        "from scheduler import create_scheduler\n",
        "from optim import create_optimizer\n",
        "\n",
        "# print and plotting\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %reload_ext autoreload"
      ],
      "metadata": {
        "id": "JiebcG8heA07"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prep data\n",
        "# download from website\n",
        "\n",
        "# make folder /content/data\n",
        "DATA_PATH = os.path.join('/content', 'data')\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "%cd /content/data\n",
        "\n",
        "# download data from links:\n",
        "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/json_pretrain.zip\n",
        "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/data.tar.gz\n",
        "# http://images.cocodataset.org/zips/train2014.zip\n",
        "# http://images.cocodataset.org/zips/val2014.zip\n",
        "# http://images.cocodataset.org/zips/test2015.zip\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the download links\n",
        "links = [\n",
        "    \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/json_pretrain.zip\",\n",
        "    \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/data.tar.gz\",\n",
        "    \"http://images.cocodataset.org/zips/train2014.zip\", # comment out if only run evaluation\n",
        "    \"http://images.cocodataset.org/zips/val2014.zip\",   # comment out if only run evaluation\n",
        "    \"http://images.cocodataset.org/zips/test2015.zip\"\n",
        "]\n",
        "\n",
        "# Download and extract each file\n",
        "for link in links:\n",
        "    filename = link.split('/')[-1]\n",
        "    print(f\"Downloading {filename}...\")\n",
        "\n",
        "    # Download file\n",
        "    !wget -q --show-progress {link}\n",
        "\n",
        "    print(f\"Extracting {filename}...\")\n",
        "\n",
        "    # Extract based on file extension\n",
        "    if filename.endswith('.zip'):\n",
        "      if '//images.cocodataset.org/zips/' in link:\n",
        "        !unzip -q {filename}\n",
        "      else:\n",
        "        !unzip -q -j {filename}  # -j option flattens the directory structure for json_pretrain.zip\n",
        "    elif filename.endswith('.tar.gz'):\n",
        "        !tar -xzf {filename} --strip-components=1  # Remove the top-level directory\n",
        "\n",
        "    # Delete the zip/tar file after extraction\n",
        "    print(f\"Removing {filename}...\")\n",
        "    !rm {filename}\n",
        "\n",
        "    print(f\"Finished processing {filename}\")\n",
        "\n",
        "print(\"All downloads and extractions completed!\")\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "kPxGkhRy4FLD",
        "outputId": "1748aa7b-bdba-4350-9f8f-25e0ca754ab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "Downloading json_pretrain.zip...\n",
            "json_pretrain.zip   100%[===================>] 630.14M   113MB/s    in 8.8s    \n",
            "Extracting json_pretrain.zip...\n",
            "Removing json_pretrain.zip...\n",
            "Finished processing json_pretrain.zip\n",
            "Downloading data.tar.gz...\n",
            "data.tar.gz         100%[===================>] 137.87M   199MB/s    in 0.7s    \n",
            "Extracting data.tar.gz...\n",
            "Removing data.tar.gz...\n",
            "Finished processing data.tar.gz\n",
            "Downloading train2014.zip...\n",
            "train2014.zip        65%[============>       ]   8.18G  55.5MB/s    eta 85s    "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/data"
      ],
      "metadata": {
        "id": "Sc9XbDXyL4WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check files\n",
        "%cd /content/data\n",
        "!ls\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "y9cbgfXjUT_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "FETCH_PRETRAINED_MODEL = True\n",
        "%cd /content\n",
        "\n",
        "# download data from links:\n",
        "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\n",
        "# https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/vqa.pth\n",
        "# model check point from training\n",
        "# https://drive.google.com/file/d/1yEsyeB0FkIgWlT2Way_KFLPNLCQy6KoU/view?usp=sharing\n",
        "\n",
        "if FETCH_PRETRAINED_MODEL:\n",
        "\n",
        "  # Define the download links\n",
        "  links = [\n",
        "      \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\",\n",
        "      \"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/vqa.pth\"\n",
        "  ]\n",
        "\n",
        "  # Download and extract each file\n",
        "  for link in links:\n",
        "      filename = link.split('/')[-1]\n",
        "      print(f\"Downloading {filename}...\")\n",
        "\n",
        "      # Download file\n",
        "      !wget -q --show-progress {link}\n",
        "\n",
        "\n",
        "      print(f\"Finished processing {filename}\")\n",
        "\n",
        "  print(\"All model downloads completed!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wWKM1ISxSrPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm5adbe9FajI"
      },
      "source": [
        "## Setup for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SSGJE2MFajI"
      },
      "outputs": [],
      "source": [
        "# config\n",
        "%cd /content\n",
        "args = argparse.Namespace()\n",
        "args.config = './configs/VQA.yaml'\n",
        "args.checkpoint = './ALBEF_4M.pth'\n",
        "args.output_dir = 'output/vqa_4MfinetuneVQA'\n",
        "args.evaluate = False # to train use False\n",
        "args.text_encoder = 'bert-base-uncased'\n",
        "args.text_decoder = 'bert-base-uncased'\n",
        "args.device = 'cuda'\n",
        "args.seed = 42\n",
        "args.distributed = False\n",
        "\n",
        "config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n",
        "pprint(config)\n",
        "\n",
        "# make result folder and save config\n",
        "args.result_dir = os.path.join(args.output_dir, 'result')\n",
        "\n",
        "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyGrj0NZFajI"
      },
      "outputs": [],
      "source": [
        "# training functions\n",
        "def train(model, data_loader, optimizer, tokenizer, epoch, warmup_steps, device, scheduler, config):\n",
        "    # train\n",
        "    model.train()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
        "\n",
        "    header = 'Train Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 50\n",
        "    step_size = 100\n",
        "    warmup_iterations = warmup_steps*step_size\n",
        "\n",
        "    for i,(image, question, answer, weights, n) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
        "        image, weights = image.to(device,non_blocking=True), weights.to(device,non_blocking=True)\n",
        "        question_input = tokenizer(question, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\").to(device)\n",
        "        answer_input = tokenizer(answer, padding='longest', return_tensors=\"pt\").to(device)\n",
        "\n",
        "        if epoch>0 or not config['warm_up']:\n",
        "            alpha = config['alpha']\n",
        "        else:\n",
        "            alpha = config['alpha']*min(1,i/len(data_loader))\n",
        "\n",
        "        loss = model(image, question_input, answer_input, train=True, alpha=alpha, k=n, weights=weights)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss=loss.item())\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        if epoch==0 and i%step_size==0 and i<=warmup_iterations:\n",
        "            scheduler.step(i//step_size)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger.global_avg())\n",
        "    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluation(model, data_loader, tokenizer, device, config) :\n",
        "    # test\n",
        "    model.eval()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Generate VQA test result:'\n",
        "    print_freq = 50\n",
        "\n",
        "    result = []\n",
        "\n",
        "    answer_list = [answer+config['eos'] for answer in data_loader.dataset.answer_list]\n",
        "    answer_input = tokenizer(answer_list, padding='longest', return_tensors='pt').to(device)\n",
        "\n",
        "    for n, (image, question, question_id) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
        "        image = image.to(device,non_blocking=True)\n",
        "        question_input = tokenizer(question, padding='longest', return_tensors=\"pt\").to(device)\n",
        "\n",
        "        topk_ids, topk_probs = model(image, question_input, answer_input, train=False, k=config['k_test'])\n",
        "\n",
        "        for ques_id, topk_id, topk_prob in zip(question_id, topk_ids, topk_probs):\n",
        "            ques_id = int(ques_id.item())\n",
        "            _, pred = topk_prob.max(dim=0)\n",
        "            result.append({\"question_id\":ques_id, \"answer\":data_loader.dataset.answer_list[topk_id[pred]]})\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_fo-oPwFajI"
      },
      "outputs": [],
      "source": [
        "# setup for training/evaluation (from main)\n",
        "utils.init_distributed_mode(args)\n",
        "\n",
        "device = torch.device(args.device)\n",
        "print(f'device: {device}')\n",
        "\n",
        "# fix the seed for reproducibility\n",
        "seed = args.seed + utils.get_rank()\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "start_epoch = 0\n",
        "max_epoch = config['schedular']['epochs']\n",
        "warmup_steps = config['schedular']['warmup_epochs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmtKe-XVFajI"
      },
      "outputs": [],
      "source": [
        "# make dataset and dataloader\n",
        "print(\"Creating vqa datasets\")\n",
        "datasets = create_dataset('vqa', config)\n",
        "\n",
        "if args.distributed:\n",
        "    num_tasks = utils.get_world_size()\n",
        "    global_rank = utils.get_rank()\n",
        "    samplers = create_sampler(datasets, [True, False], num_tasks, global_rank)\n",
        "else:\n",
        "    samplers = [None, None]\n",
        "\n",
        "train_loader, test_loader = create_loader(datasets,samplers,\n",
        "                                          batch_size=[config['batch_size_train'],config['batch_size_test']],\n",
        "                                          num_workers=[4,4],is_trains=[True, False],\n",
        "                                          collate_fns=[vqa_collate_fn,None])\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(args.text_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zYcjMhhFajI"
      },
      "outputs": [],
      "source": [
        "#### Model ####\n",
        "print(\"Creating model\")\n",
        "model = ALBEF(config=config, text_encoder=args.text_encoder, text_decoder=args.text_decoder, tokenizer=tokenizer)\n",
        "model = model.to(device)\n",
        "\n",
        "arg_opt = utils.AttrDict(config['optimizer'])\n",
        "optimizer = create_optimizer(arg_opt, model)\n",
        "arg_sche = utils.AttrDict(config['schedular'])\n",
        "lr_scheduler, _ = create_scheduler(arg_sche, optimizer)\n",
        "\n",
        "# check model\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B64EarwbFajJ"
      },
      "outputs": [],
      "source": [
        "# load check point to continue training\n",
        "if args.checkpoint:\n",
        "    checkpoint = torch.load(args.checkpoint, map_location='cpu')\n",
        "    if args.evaluate:\n",
        "        state_dict = checkpoint\n",
        "    else:\n",
        "        state_dict = checkpoint['model']\n",
        "\n",
        "    # reshape positional embedding to accomodate for image resolution change\n",
        "    pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)\n",
        "    state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
        "    # # Check if the key exists before accessing it\n",
        "    # if 'visual_encoder.pos_embed' in state_dict:\n",
        "    #     # reshape positional embedding to accomodate for image resolution change\n",
        "    #     pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)\n",
        "    #     state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
        "    # else:\n",
        "    #     print(\"Warning: 'visual_encoder.pos_embed' not found in checkpoint. Skipping positional embedding interpolation.\")\n",
        "\n",
        "\n",
        "    if not args.evaluate:\n",
        "        if config['distill']:\n",
        "            m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)\n",
        "            state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped\n",
        "\n",
        "        for key in list(state_dict.keys()):\n",
        "            if 'bert' in key:\n",
        "                encoder_key = key.replace('bert.','')\n",
        "                state_dict[encoder_key] = state_dict[key]\n",
        "            # intialize text decoder as multimodal encoder (last 6 layers of model.text_encoder)\n",
        "            if 'text_encoder' in key:\n",
        "                if 'layer' in key:\n",
        "                    # print(key)\n",
        "                    encoder_keys = key.split('.')\n",
        "                    # print(encoder_keys)\n",
        "                    # print(encoder_keys[4])\n",
        "                    tmp_fix_idx = 5 # for the downsized model, idx 5 is the layer number\n",
        "                    layer_num = int(encoder_keys[tmp_fix_idx]) # 4\n",
        "                    if layer_num<6:\n",
        "                        del state_dict[key]\n",
        "                        continue\n",
        "                    else:\n",
        "                        decoder_layer_num = (layer_num-6)\n",
        "                        encoder_keys[4] = str(decoder_layer_num)\n",
        "                        encoder_key = '.'.join(encoder_keys)\n",
        "                else:\n",
        "                    encoder_key = key\n",
        "                decoder_key = encoder_key.replace('text_encoder','text_decoder')\n",
        "                state_dict[decoder_key] = state_dict[key]\n",
        "\n",
        "                del state_dict[key]\n",
        "\n",
        "    msg = model.load_state_dict(state_dict,strict=False)\n",
        "    print('load checkpoint from %s'%args.checkpoint)\n",
        "    print(msg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJmSxRhtFajJ"
      },
      "outputs": [],
      "source": [
        "# handle distributed training\n",
        "model_without_ddp = model\n",
        "if args.distributed:\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "    model_without_ddp = model.module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyQyvZE1FajJ"
      },
      "outputs": [],
      "source": [
        "# training loop, single GPU\n",
        "print(\"Start training\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, max_epoch):\n",
        "    if epoch>0:\n",
        "        lr_scheduler.step(epoch+warmup_steps)\n",
        "\n",
        "    if not args.evaluate:\n",
        "        if args.distributed:\n",
        "            train_loader.sampler.set_epoch(epoch)\n",
        "\n",
        "        train_stats = train(model, train_loader, optimizer, tokenizer, epoch, warmup_steps, device, lr_scheduler, config)\n",
        "\n",
        "    if args.evaluate:\n",
        "        break\n",
        "\n",
        "    if utils.is_main_process():\n",
        "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                      'epoch': epoch,\n",
        "                    }\n",
        "        with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
        "            f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "        save_obj = {\n",
        "            'model': model_without_ddp.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'lr_scheduler': lr_scheduler.state_dict(),\n",
        "            'config': config,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_%02d.pth'%epoch))\n",
        "    if args.distributed:\n",
        "        dist.barrier()\n",
        "    else:\n",
        "        pass  # Skip barrier for non-distributed training\n",
        "\n",
        "# evaluation\n",
        "vqa_result = evaluation(model, test_loader, tokenizer, device, config)\n",
        "result_file = save_result(vqa_result, args.result_dir, 'vqa_result_epoch%d'%epoch)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "print('Time time {}'.format(total_time_str))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_DRIVE_PATH"
      ],
      "metadata": {
        "id": "T52hIhCtaoJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save result to google drive\n",
        "!cp -r {args.output_dir} {GOOGLE_DRIVE_PATH}"
      ],
      "metadata": {
        "id": "qmVYMgYzgka2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}